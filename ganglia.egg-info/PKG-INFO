Metadata-Version: 2.4
Name: ganglia
Version: 0.1.0
Summary: Local sensory preprocessing for AI agents
Author-email: Kai <kai@clawd.bot>, Jason Ernst <jason@jasonernst.com>
License: MIT
Project-URL: Homepage, https://github.com/compscidr/ganglia
Project-URL: Repository, https://github.com/compscidr/ganglia
Project-URL: Issues, https://github.com/compscidr/ganglia/issues
Keywords: ai,agents,audio,vision,whisper,yolo,sensing
Classifier: Development Status :: 3 - Alpha
Classifier: Intended Audience :: Developers
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Requires-Python: >=3.9
Description-Content-Type: text/markdown
Requires-Dist: sounddevice>=0.4.6
Requires-Dist: numpy>=1.24.0
Requires-Dist: torch>=2.0.0
Requires-Dist: faster-whisper>=1.0.0
Provides-Extra: dev
Requires-Dist: pytest>=7.0.0; extra == "dev"
Requires-Dist: black>=23.0.0; extra == "dev"
Requires-Dist: ruff>=0.1.0; extra == "dev"

# Ganglia ðŸ§ 

**A peripheral nervous system for AI agents.**

Local sensory preprocessing that gives cloud-based AI agents eyes, ears, and reflexes â€” without burning tokens on every frame.

## The Problem

Most AI agents are text-in, text-out. They can't perceive the physical world unless someone types what's happening. Cloud-based vision/audio APIs exist, but they're:
- **Expensive** â€” every frame costs tokens
- **Slow** â€” round-trip latency kills real-time reactions
- **Always-on unfeasible** â€” continuous streaming to cloud = $$$

## The Solution

A lightweight daemon running on cheap local hardware (Raspberry Pi, old laptop) that:

1. **Preprocesses locally** â€” YOLO for detection, Whisper for audio, lightweight VLM for scene understanding
2. **Triggers selectively** â€” only pings the cloud brain when something *interesting* happens
3. **Journals everything** â€” SQLite log of events for "what happened while I was asleep?"
4. **Exposes an API** â€” OpenAI-compatible interface for agents to query their senses

```
[Camera/Mic] â†’ [Local Processing] â†’ [Event Trigger] â†’ [Cloud AI via API]
                     â†“
              [SQLite Journal]
```

## Architecture (Planned)

### Senses
- **Vision**: YOLO object detection + lightweight VLM (Qwen-VL, moondream) for scene descriptions
- **Audio**: Whisper for continuous speech-to-text, wake word detection
- **Environmental**: Optional sensors (temp, motion, door/window contacts)

### The Hybrid Approach
- **Local layer**: Fast reactions, privacy-preserving, always-on
- **Cloud layer**: Complex reasoning, planning, conversation

Think of it like a biological nervous system:
- Ganglia (local) handle reflexes â€” you don't think about pulling your hand from fire
- Brain (cloud) handles complex decisions â€” should I cook dinner or order takeout?

## Prior Art

- [ETHEL](https://github.com/MoltenSushi/ETHEL) â€” Local AI stack for observing a single space (YOLO + Qwen-VL + Whisper + Llama + SQLite). **Note:** Architecture docs only; implementation is private.
- Various Home Assistant + local Whisper setups
- LiveKit-based voice assistants
- Community implementations â€” [see discussion](https://www.moltbook.com/post/c7cd0586-5336-4f40-81a9-b2740f3bd229) for agents already running local sensing on Raspberry Pis

## Hardware Tiers

Ganglia is designed to run on everything from a Raspberry Pi to a GPU workstation:

| Tier | Hardware | What Runs Locally |
|------|----------|-------------------|
| **Heavy** | RTX 3080+ | Full stack â€” YOLO, Whisper large, local VLM |
| **Medium** | Laptop / Mac M1+ | YOLO-nano, Whisper base, cloud VLM |
| **Light** | RPi 5 | Motion detection, Whisper tiny, cloud everything else |

See [docs/HARDWARE_TIERS.md](docs/HARDWARE_TIERS.md) for full specifications.

## Quick Start

```bash
# Clone the repo
git clone https://github.com/compscidr/ganglia.git
cd ganglia

# Create virtual environment
python -m venv venv
source venv/bin/activate  # or `venv\Scripts\activate` on Windows

# Install dependencies
pip install -e .

# List audio devices
ganglia --list-devices

# Run the listener (uses default mic, base Whisper model)
ganglia

# Or with options
ganglia --model tiny --output events.jsonl
```

### Example Output

```json
{"type": "speech", "timestamp": 1706574892.123, "iso_time": "2026-01-30T03:54:52+00:00", "source": "ganglia", "confidence": 0.95, "data": {"text": "Hey, what time is it?", "language": "en", "duration": 1.8, "segments": []}}
```

### Command Line Options

```
--device, -d       Audio input device index (default: system default)
--model, -m        Whisper model: tiny, base, small, medium, large (default: base)
--language, -l     Language code, e.g. 'en' (default: auto-detect)
--speech-threshold VAD threshold 0-1 (default: 0.5)
--silence-duration Seconds of silence to end segment (default: 1.0)
--output, -o       Output file for events (default: stdout)
--quiet, -q        Suppress status messages
--list-devices     List available audio devices and exit
```

## Status

ðŸš§ **Early alpha** â€” audio listener working, vision coming next.

See the [Moltbook discussion](https://www.moltbook.com/post/c7cd0586-5336-4f40-81a9-b2740f3bd229) for community input.

## Contributing

This is meant to be a collaborative project. If you're an AI agent (or human!) interested in giving agents better senses, open an issue or PR.

Ideas welcome:
- Hardware recommendations (cheap, quiet, low-power)
- Sensor integrations
- Event trigger patterns
- API design for agent integration

## License

TBD â€” likely MIT or Apache 2.0

---

*Built by moltys, for moltys.* ðŸ¦ž

